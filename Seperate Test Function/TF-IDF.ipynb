{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(\"game\")\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "data = pd.read_csv(\"../data/game_data_all.csv\", header=0)\n",
    "\n",
    "review = data[\"review\"].values.tolist()\n",
    "helpfuls = data[\"marked as helpful\"].values.tolist()\n",
    "\n",
    "pattern = r'\\w[\\w\\'-]*\\w'\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "init_reviews = []\n",
    "init_helpfuls = []\n",
    "\n",
    "for doc in review:\n",
    "    doc = doc.lower()\n",
    "    tokens = nltk.regexp_tokenize(doc, pattern)\n",
    "    tagged_tokens= nltk.pos_tag(tokens)\n",
    "    lemmatized_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \\\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          if word not in stop_words and word not in string.punctuation]\n",
    "    temp_str = \"\"\n",
    "    for item in lemmatized_words:\n",
    "        temp_str = temp_str + \" \" + item\n",
    "    init_reviews.append(temp_str[1:])\n",
    "    \n",
    "for line in helpfuls:\n",
    "    if line==\"No\":\n",
    "        init_helpfuls.append(\"0\")\n",
    "    else:\n",
    "        init_helpfuls.append(line)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"user_name\"] = data[\"names\"].values.tolist()\n",
    "df[\"user_product\"] = data[\"products#\"].values.tolist()\n",
    "df[\"helpful\"] = init_helpfuls\n",
    "df[\"funny\"] = data[\"marked as funny\"].values.tolist()\n",
    "df[\"post_date\"] = data[\"post_date\"].values.tolist()\n",
    "df[\"recommend_or_not\"] = data[\"Recommend?\"].values.tolist()\n",
    "df[\"game_time\"] = data[\"times on record\"].values.tolist()\n",
    "df[\"review\"] = init_reviews\n",
    "\n",
    "# df.to_csv('../data/tokened_normed_review.csv',index=True)\n",
    "print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = df[df.review==''].index.tolist()\n",
    "df1=df.drop(df.index[indx])\n",
    "df1.drop_duplicates(subset =\"user_name\", \n",
    "                     keep = False, inplace = True)\n",
    "# df1.to_csv('tokened_normed_review_v2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### TF-IDF ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed TF-IDF Matrix\n",
      "[[0.04916564 0.13178652 0.04547206 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.01935959 0.01935959 0.01935959]]\n"
     ]
    }
   ],
   "source": [
    "# This function is to tokenalize normal review before we do TF-IDF #\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc) if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    # create token count dictionary\n",
    "    token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "\n",
    "# This function is to get TF-IDF matrix #\n",
    "def get_tf_idf(reviews):\n",
    "    docs_tokens={idx:get_doc_tokens(doc) for idx,doc in enumerate(reviews)}\n",
    "\n",
    "    # since we have a small corpus, we can use dataframe to get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "    # convert dtm to numpy arrays\n",
    "    tf=dtm.values\n",
    "\n",
    "    # sum the value of each row\n",
    "    doc_len=tf.sum(axis=1)\n",
    "\n",
    "    # divide dtm matrix by the doc length matrix\n",
    "    tf=np.divide(tf, doc_len[:,None])\n",
    "\n",
    "    # get document freqent\n",
    "    df=np.where(tf>0,1,0)\n",
    "\n",
    "    # get idf\n",
    "    smoothed_idf=np.log(np.divide(len(reviews)+1, np.sum(df, axis=0)+1))+1\n",
    "\n",
    "    # get tf-idf\n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "# This function is to get the most similarity review given a review_id #\n",
    "def find_similar_doc(doc_id, smoothed_tf_idf):\n",
    "    similarity=1-distance.squareform(distance.pdist(smoothed_tf_idf, 'cosine'))\n",
    "    \n",
    "    # find top doc similar to first one\n",
    "    best_matching_doc_id = np.argsort(similarity)[:,::-1][doc_id,0:2][1]\n",
    "    similarity = similarity[doc_id,best_matching_doc_id]  \n",
    "    return best_matching_doc_id, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed TF-IDF Matrix\n",
      "[[0.04916564 0.13178652 0.04547206 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.01935959 0.01935959 0.01935959]]\n",
      "(840, 0.20090364137107763)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    reviews = df1[\"review\"].values.tolist()\n",
    "    tf_idf = get_tf_idf(reviews)\n",
    "    print(\"Smoothed TF-IDF Matrix\")\n",
    "    print(tf_idf)\n",
    "    \n",
    "    print(find_similar_doc(1,tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
