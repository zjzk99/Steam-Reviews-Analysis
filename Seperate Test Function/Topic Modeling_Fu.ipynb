{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from numpy.random import shuffle\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic(file_path):\n",
    "    raw_df = pd.read_csv(file_path)\n",
    "\n",
    "    #raw_df.user_product=raw_df.user_product.apply(lambda x :x.replace(',','')).astype(int)\n",
    "    #raw_df.helpful=raw_df.helpful.apply(lambda x :x.replace(',','')).astype(int)\n",
    "    #raw_df.funny=raw_df.funny.apply(lambda x :x.replace(',','')).astype(int)\n",
    "    #raw_df.game_time=raw_df.game_time.apply(lambda x :x.replace(',','')).astype(float)\n",
    "    raw_df['review_len']=raw_df.review.apply(lambda x: len(x))\n",
    "\n",
    "    #filter_condition = (raw_df.game_time > 60) & (raw_df.helpful > 0) & (raw_df.review_len > 10)\n",
    "    #filtered_df = raw_df[filter_condition]\n",
    "    filtered_df = raw_df\n",
    "    filtered_df.shape\n",
    "\n",
    "    # get bigrams\n",
    "    text = []\n",
    "    tagged_tokens = []\n",
    "    for doc in filtered_df['review']:\n",
    "        tokens=nltk.word_tokenize(doc)\n",
    "        temp_tokens= nltk.pos_tag(tokens)\n",
    "        for x in temp_tokens:\n",
    "            tagged_tokens.append(x)\n",
    "\n",
    "    bigrams=list(nltk.bigrams(tagged_tokens))\n",
    "    for (x,y) in bigrams:\n",
    "        if x[1].startswith('J') and y[1].startswith('N'):\n",
    "            text.append(x[0] + \" \" + y[0])\n",
    "            \n",
    "            \n",
    "    #text = list(filtered_df['review'])\n",
    "    print(text[0])\n",
    "    print(tagged_tokens[0])\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    other_words = [\"game\",'divinity','original',\"sin\",'play','use','make','really','thing','get','go','would','early','access']\n",
    "    stop_words += other_words\n",
    "    \n",
    "    # create tf-idf\n",
    "    tf_vectorizer = CountVectorizer(max_df = 0.90, min_df = 5, stop_words = stop_words)\n",
    "    tf = tf_vectorizer.fit_transform(text)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print(tf_feature_names[0:10])\n",
    "    print(tf.shape)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    tf_train, tf_test = train_test_split(tf, test_size=0.3, random_state=0)\n",
    "    \n",
    "    \n",
    "    num_topics = 10\n",
    "\n",
    "    # Run LDA. For details, check\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.perplexity\n",
    "\n",
    "    # max_iter control the number of iterations \n",
    "    # evaluate_every determines how often the perplexity is calculated\n",
    "    # n_jobs is the number of parallel threads\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                max_iter=30,verbose=1,\n",
    "                                evaluate_every=1, n_jobs=1,\n",
    "                                random_state=0).fit(tf_train)\n",
    "    \n",
    "    \n",
    "    # lda for test set\n",
    "    topic_assign_test = lda.transform(tf_test)\n",
    "\n",
    "    # find the topic with top one highest possibility\n",
    "    top_one_topic = [doc.argsort()[-1] for doc in topic_assign_test]\n",
    "\n",
    "    top_one_topic\n",
    "    \n",
    "    \n",
    "    num_top_words=20\n",
    "\n",
    "    # lda.components_ returns a KxN matrix\n",
    "    # for word distribution in each topic.\n",
    "    # Each row consists of \n",
    "    # probability (counts) of each word in the feature space\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        #print (\"Topic %d:\" % (topic_idx))\n",
    "        # print out top 20 words per topic \n",
    "        words=[(tf_feature_names[i],topic[i]) \\\n",
    "               for i in topic.argsort()[::-1][0:num_top_words]]\n",
    "        #print(words)\n",
    "        #print(\"\\n\")\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    num_top_words=50\n",
    "    f, axarr = plt.subplots(5, 2, figsize=(20, 20));\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        # create a dataframe with two columns (word, weight) for each topic\n",
    "\n",
    "        # create a word:count dictionary\n",
    "        f={tf_feature_names[i]:topic[i] for i in topic.argsort()[::-1][0:num_top_words]}\n",
    "\n",
    "        # generate wordcloud in subplots\n",
    "        wordcloud = WordCloud(width=480, height=450, margin=0, background_color=\"black\");\n",
    "        _ = wordcloud.generate_from_frequencies(frequencies=f);\n",
    "\n",
    "        _ = axarr[math.floor(topic_idx/2), topic_idx%2].imshow(wordcloud, interpolation=\"bilinear\");\n",
    "        _ = axarr[math.floor(topic_idx/2), topic_idx%2].set_title(\"Topic: \"+str(topic_idx));\n",
    "        _ = axarr[math.floor(topic_idx/2), topic_idx%2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ = '__main__':\n",
    "    file_path1 = 'Patch_1.csv'\n",
    "    file_path2 = 'Patch_2.csv'\n",
    "    file_path3 = 'Patch_3.csv'\n",
    "    file_path4 = 'Patch_4.csv'\n",
    "    topic(file_path1)\n",
    "    topic(file_path2)\n",
    "    topic(file_path3)\n",
    "    topic(file_path4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
